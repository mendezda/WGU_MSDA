{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4300fe10-801f-44f9-bd23-3f65ff7e362a",
   "metadata": {},
   "source": [
    "# Part I: Research Question \n",
    "\n",
    "### A. Research Question: \n",
    "The data set selected for this performance assessment is the **churn** data set. The research question for this assessment is:  \n",
    "\n",
    "*What customer attributes (all variables from **demographic data**, **personal information data**, **account information and services data**, defined below)  contribute to whether a customer will discontinue their service?*\n",
    "\n",
    "### B. Description of Variables:\n",
    "\n",
    "In the churn data set, there are a total of 10000 rows representing 10000 customers. According to the data dictionary, there are 50 variables. However, although the data dictionary defines 'Lat, Lng' as one variable, the data frame appears to have two distinct variables, 'Lat' and 'Lng'. Furthermore, there appears to be an unnamed first column which indexes the data set, which is not referenced in the data dictionary.\n",
    "\n",
    "The following variables represent various **customer IDs**:  \n",
    "\n",
    "- unnamed column: An index column without a title (**nominal categorical data**)\n",
    "- CaseOrder: A placeholder variable to preserve the original order of the raw data (**nominal categorical data**)\n",
    "- Customer_id: Unique customer ID (**nominal categorical data**)\n",
    "- Interaction, UID: Unique IDs related to customer interactions (**nominal categorical data**)\n",
    "\n",
    "The following variables represent customer **demographic data**: \n",
    "\n",
    "- City: Customer city of residence (**nominal categorical data**)\n",
    "- State: Customer state of residence (**nominal categorical data**)\n",
    "- County: Customer county of residence (**nominal categorical data**)\n",
    "- Zip: Customer zip code of residence (**nominal categorical data**)\n",
    "- Lat: GPS coordinates of the latitude of the customer residence (**continuous numeric data**)\n",
    "- Lng: GPS coordinates of the longitude of the customer residence (**continuous numeric data**)\n",
    "- Population: Population within a mile radius of customer (**discrete numeric data**)\n",
    "- Area: Area type (rural, urban, suburban) (**nominal categorical data**)\n",
    "- TimeZone: Time zone of customer residence based on customer sign-up information (**nominal categorical data**)\n",
    "\n",
    "The following variables represent customer **personal information data**:  \n",
    "\n",
    "- Job: Job of the customer/invoiced person (**nominal categorical data**)\n",
    "- Children: Number of children in customer’s household (**discrete numeric data**)\n",
    "- Age: Age of customer (**continuous numeric data**)\n",
    "- Education: Highest degree earned by customer (**ordinal categorical data**)\n",
    "- Employment: Employment status of customer (**nominal categorical data**)\n",
    "- Income: Annual income of customer (**continuous numeric data**)\n",
    "- Marital: Marital status of customer (**nominal categorical data**)\n",
    "- Gender: Customer self-identification as male, female, or nonbinary (**nominal categorical data**)\n",
    "\n",
    "The following variables represent customer **account information and services data**:  \n",
    "\n",
    "- Churn: Whether the customer discontinued service within the last month (yes, no) (**binary nominal categorical data**)\n",
    "- Outage_sec_perweek: Average number of seconds per week of system outages in the customer’s neighborhood (**continuous numeric data**)\n",
    "- Email: Number of emails sent to the customer in the last year (marketing or correspondence) (**discrete numeric data**)\n",
    "- Contacts: Number of times customer contacted technical support (**discrete numeric data**)\n",
    "- Yearly_equip_failure: The number of times customer’s equipment failed and had to be reset/replaced in the past year (**discrete numeric data**)\n",
    "- Techie: Whether the customer considers themselves technically inclined (yes, no) (**binary nominal categorical data**)\n",
    "- Contract: The contract term of the customer (month-to-month, one year, two year) (**nominal categorical data**)\n",
    "- Port_modem: Whether the customer has a portable modem (yes, no) (**binary nominal categorical data**)\n",
    "- Tablet: Whether the customer owns a tablet such as iPad, Surface, etc. (yes, no) (**binary nominal categorical data**)\n",
    "- InternetService: Customer’s internet service provider (DSL, fiber optic, None) (**nominal categorical data**)\n",
    "- Phone: Whether the customer has a phone service (yes, no) (**binary nominal categorical data**)\n",
    "- Multiple: Whether the customer has multiple lines (yes, no) (**binary nominal categorical data**)\n",
    "- OnlineSecurity: Whether the customer has an online security add-on (yes, no) (**binary nominal categorical data**)\n",
    "- OnlineBackup: Whether the customer has an online backup add-on (yes, no) (**binary nominal categorical data**)\n",
    "- DeviceProtection: Whether the customer has device protection add-on (yes, no) (**binary nominal categorical data**)\n",
    "- TechSupport: Whether the customer has a technical support add-on (yes, no) (**binary nominal categorical data**)\n",
    "- StreamingTV: Whether the customer has streaming TV (yes, no) (**binary nominal categorical data**)\n",
    "- StreamingMovies: Whether the customer has streaming movies (yes, no) (**binary nominal categorical data**)\n",
    "- PaperlessBilling: Whether the customer has paperless billing (yes, no) (**binary nominal categorical data**)\n",
    "- PaymentMethod: The customer’s payment method (electronic check, mailed check, bank (automatic bank transfer), credit card (automatic)) (**nominal categorical data**)\n",
    "- Tenure: Number of months the customer has stayed with the provider (**continuous numeric data**)\n",
    "- MonthlyCharge: The amount charged, on average, per customer monthly (**continuous numeric data**)\n",
    "- Bandwidth_GB_Year: The average amount of data used, in GB, in a year by the customer (**continuous numeric data**)\n",
    "\n",
    "The following variables represent customers' responses to an eight-question survey, with responses on a scale of 1 to 8 (1 = most important, 8 = least important)  \n",
    "\n",
    "- Item1: Timely response (**ordinal categorical data**)\n",
    "- Item2: Timely fixes (**ordinal categorical data**)\n",
    "- Item3: Timely replacements (**ordinal categorical data**)\n",
    "- Item4: Reliability (**ordinal categorical data**)\n",
    "- Item5: Options (**ordinal categorical data**)\n",
    "- Item6: Respectful response (**ordinal categorical data**)\n",
    "- Item7: Courteous exchange (**ordinal categorical data**)\n",
    "- Item8: Evidence of active listening (**ordinal categorical data**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7af5b3",
   "metadata": {},
   "source": [
    "### D208\n",
    "\n",
    "**10 omitted from model**\n",
    "- CaseOrder: A placeholder variable to preserve the original order of the raw data (**nominal categorical data**)\n",
    "- Customer_id: Unique customer ID (**nominal categorical data**)\n",
    "- Interaction: Unique IDs related to customer interactions (**nominal categorical data**)\n",
    "- UID: Unique IDs related to customer interactions (**nominal categorical data**)\n",
    "- City: Customer city of residence (**nominal categorical data**)\n",
    "- State: Customer state of residence (**nominal categorical data**)\n",
    "- County: Customer county of residence (**nominal categorical data**)\n",
    "- Zip: Customer zip code of residence (**nominal categorical data**)\n",
    "- Job: Job of the customer/invoiced person (**nominal categorical data**)\n",
    "- TimeZone: Time zone of customer residence based on customer sign-up information (**nominal categorical data**)\n",
    "\n",
    "\n",
    "**13 numeric**\n",
    "- Lat: GPS coordinates of the latitude of the customer residence (**continuous numeric data**)\n",
    "- Lng: GPS coordinates of the longitude of the customer residence (**continuous numeric data**)\n",
    "- Population: Population within a mile radius of customer (**discrete numeric data**)\n",
    "- Children: Number of children in customer’s household (**discrete numeric data**)\n",
    "- Age: Age of customer (**continuous numeric data**)\n",
    "- Income: Annual income of customer (**continuous numeric data**)\n",
    "- Outage_sec_perweek: Average number of seconds per week of system outages in the customer’s neighborhood (**continuous numeric data**)\n",
    "- Email: Number of emails sent to the customer in the last year (marketing or correspondence) (**discrete numeric data**)\n",
    "- Contacts: Number of times customer contacted technical support (**discrete numeric data**)\n",
    "- Yearly_equip_failure: The number of times customer’s equipment failed and had to be reset/replaced in the past year (**discrete numeric data**)\n",
    "- Tenure: Number of months the customer has stayed with the provider (**continuous numeric data**)\n",
    "- MonthlyCharge: The amount charged, on average, per customer monthly (**continuous numeric data**)\n",
    "- Bandwidth_GB_Year: The average amount of data used, in GB, in a year by the customer (**continuous numeric data**)\n",
    "\n",
    "\n",
    "**6 one-hot encoding**\n",
    "- Area: Area type (rural, urban, suburban) (**nominal categorical data**)\n",
    "- Marital: Marital status of customer (**nominal categorical data**)\n",
    "- Gender: Customer self-identification as male, female, or nonbinary (**nominal categorical data**)\n",
    "- Contract: The contract term of the customer (month-to-month, one year, two year) (**nominal categorical data**)\n",
    "- InternetService: Customer’s internet service provider (DSL, fiber optic, None) (**nominal categorical data**)\n",
    "- PaymentMethod: The customer’s payment method (electronic check, mailed check, bank (automatic bank transfer), credit card (automatic)) (**nominal categorical data**)\n",
    "\n",
    "\n",
    "**13 binary encoded**\n",
    "- Churn: Whether the customer discontinued service within the last month (yes, no) (**binary nominal categorical data**)\n",
    "- Techie: Whether the customer considers themselves technically inclined (yes, no) (**binary nominal categorical data**)\n",
    "- Port_modem: Whether the customer has a portable modem (yes, no) (**binary nominal categorical data**)\n",
    "- Tablet: Whether the customer owns a tablet such as iPad, Surface, etc. (yes, no) (**binary nominal categorical data**)\n",
    "- Phone: Whether the customer has a phone service (yes, no) (**binary nominal categorical data**)\n",
    "- Multiple: Whether the customer has multiple lines (yes, no) (**binary nominal categorical data**)\n",
    "- OnlineSecurity: Whether the customer has an online security add-on (yes, no) (**binary nominal categorical data**)\n",
    "- OnlineBackup: Whether the customer has an online backup add-on (yes, no) (**binary nominal categorical data**)\n",
    "- DeviceProtection: Whether the customer has device protection add-on (yes, no) (**binary nominal categorical data**)\n",
    "- TechSupport: Whether the customer has a technical support add-on (yes, no) (**binary nominal categorical data**)\n",
    "- StreamingTV: Whether the customer has streaming TV (yes, no) (**binary nominal categorical data**)\n",
    "- StreamingMovies: Whether the customer has streaming movies (yes, no) (**binary nominal categorical data**)\n",
    "- PaperlessBilling: Whether the customer has paperless billing (yes, no) (**binary nominal categorical data**)\n",
    "\n",
    "**8 ordinal** \n",
    "- Item1: Timely response (**ordinal categorical data**)\n",
    "- Item2: Timely fixes (**ordinal categorical data**)\n",
    "- Item3: Timely replacements (**ordinal categorical data**)\n",
    "- Item4: Reliability (**ordinal categorical data**)\n",
    "- Item5: Options (**ordinal categorical data**)\n",
    "- Item6: Respectful response (**ordinal categorical data**)\n",
    "- Item7: Courteous exchange (**ordinal categorical data**)\n",
    "- Item8: Evidence of active listening (**ordinal categorical data**)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcdb85f-e2a9-4f3b-966d-e2d4eaa279aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### B1. Example from the Data Set\n",
    "\n",
    "As an example of the data set, consider the 40th observation in the data set:\n",
    "\n",
    "At the time of registration, the 40th observation is a divorced male from Bucks County in Dublin, PA, 18917, which has a population of 2123. He holds a regular high school diploma, and he is employed full-time as an editorial assistant, making $50,336.50 annually. He had not discontinued his service within the last month.\n",
    "\n",
    "\n",
    "| Variable | Value of 40th Observation|\n",
    "|----------|----------|\n",
    "|  CaseOrder  |  40  |\n",
    "|  Customer_id  |  Z666770  |\n",
    "|  Interaction  |  e8e28d51-c371-4a39-a81c-0c08d2f1f6ba  |\n",
    "|  City  |  Dublin  |\n",
    "|  State  |  PA  |\n",
    "|  County  |  Bucks  |\n",
    "|  Zip  |  18917  |\n",
    "|  Lat  |  40.37305  |\n",
    "|  Lng  |  -75.2041  |\n",
    "|  Population  |  2123  |\n",
    "|  Area  |  Suburban  |\n",
    "|  Timezone  |  America/New_York  |\n",
    "|  Job  |  Editorial assistant  |\n",
    "|  Children  |  1.0  |\n",
    "|  Age  |  72.0  |\n",
    "|  Education  |  Regular High School Diploma  |\n",
    "|  Employment  |  Full Time  |\n",
    "|  Income  |  50336.5  |\n",
    "|  Marital  |  Divorced  |\n",
    "|  Gender  |  Male  |\n",
    "|  Churn  |  No  |\n",
    "|  Outage_sec_perweek  |  7.790281  |\n",
    "|  Email  |  10  |\n",
    "|  Contacts  |  1  |\n",
    "|  Yearly_equip_failure  |  1  |\n",
    "|  Techie  |  No  |\n",
    "|  Contract  |  Two Year  |\n",
    "|  Port_modem  |  Yes  |\n",
    "|  Tablet  |  No  |\n",
    "|  InternetService  |  Fiber Optic  |\n",
    "|  Phone  |  Yes  |\n",
    "|  Multiple  |  No  |\n",
    "|  OnlineSecurity  |  No  |\n",
    "|  OnlineBackup  |  Yes  |\n",
    "|  DeviceProtection  |  Yes  |\n",
    "|  TechSupport  |  No  |\n",
    "|  StreamingTV  |  No  |\n",
    "|  StreamingMovies  |  No  |\n",
    "|  PaperlessBilling  |  No  |\n",
    "|  PaymentMethod  |  Electronic Check  |\n",
    "|  Tenure  |  16.042022  |\n",
    "|  MonthlyCharge  |  147.291188  |\n",
    "|  Bandwidth_GB_Year  |  1530.10769  |\n",
    "|  item1  |  3  |\n",
    "|  item2  |  4  |\n",
    "|  item3  |  3  |\n",
    "|  item4  |  6  |\n",
    "|  item5  |  4  |\n",
    "|  item6  |  4  |\n",
    "|  item7  |  6  |\n",
    "|  item8  |  2  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42481b87-69a6-47c5-8191-c542069e692e",
   "metadata": {},
   "source": [
    "# Part II: Data-Cleaning Plan\n",
    "\n",
    "### C1. Plan to Clean the Data  \n",
    "The plan to assess the quality of the data in the data set involves detecting duplicates, missing values, and outliers, and the re-expression of a categorical variable using ordinal encoding. The steps and techniques necessary to assess the quality of the data is given:  \n",
    "\n",
    "- Import: the data will first be imported into a data frame using the `read_csv()` function from the Pandas library.\n",
    "- Duplicates:\n",
    "    - Duplicates will be detected by chaining the `.duplicated()` and `.sum()` methods from the Pandas library and calling them on the data frame, returning the total count of duplicate observations. Duplicate columns will be detected using Pandas' `.equals()` method, accessing the unnamed column using Pandas' `.iloc` indexer.\n",
    "- Missing Values:\n",
    "    - Missing values will be detected by chaining the `.isnull()` and `.sum()` methods from the Pandas library and calling them on the data frame, returning the total count of missing values for each variable.\n",
    "- Outliers:\n",
    "  - Outliers of quantitative variables will be identified using the `boxplot()` function from the Seaborn library. \n",
    "- Re-expression of Categorical Variables: \n",
    "    - The churn data dictionary was used to identify which variables required re-expression. By identifying the possible options for each categorical variable, it can be determined which variables are considered ordinal and could therefore be re-expressed using ordinal encoding.\n",
    "\n",
    "### C2. Justification of the Approach for Assessing Data Quality  \n",
    "\n",
    "- The data set contains 10000 observations, so it was necessary to check for duplicates. An efficient way to do so is with Pandas' `.duplicated()` and `.sum()` methods, which when chained, return the sum of the binary series produced by `.duplicated()`, thus counting the total number of duplicates.\n",
    "- To verify that the first unnamed column and the **CaseOrder** variable are identical, Pandas' `.equals()` method will be used, as this method efficiently checks for element-wise equality between the two objects being compared, and will only return `True` if all are equal. It was necessary to access the unnamed column with Pandas' `.iloc` indexer because the unnamed column could not be accessed via its column name.\n",
    "- Since it is necessary to check all variables for missing values, an effective way to do so utilized `.isnull()` and `.sum()`. When chained and called on the data frame, this will return a series with the sum of all missing values for each variable in the data frame.\n",
    "- The boxplot is a quick way to visualize and examine outliers of a data set, and Seaborn's `boxplot()` function provides an equally quick way to generate these plots for ease of visualization of the outliers.\n",
    "- It is only necessary to consult the data dictionary in order to ascertain which categorical variables require re-expression.\n",
    "\n",
    "### C3. Justification of Programming Language and Libraries/Packages  \n",
    "\n",
    "- Python was selected for this PA for ease of use, readability, and due to its widespread use in data analytics. It was also selected for its ability to handle large data sets efficiently.\n",
    "- In order to clean the data set, the following libraries/packages will be utilized:\n",
    "    - **Pandas**: Providing many of the tools used here, Pandas is an essential library because it provides the methods `.isnull()`, `.duplicated()`, `.equals()`, and `.sum()`, which provide important basic functionality that can be used effectively here.\n",
    "    - **Seaborn**: Seaborn will be used to generate boxplots, which enables observation of the outliers of quantitative variables.\n",
    "    - `pyplot` from **Matplotlib**: Matplotlib's `pyplot` will be used to generate histograms of the variables, which is a simple and efficient method of observing the distribution of each variable, to aid in univariate imputation.\n",
    "    - **Numpy**: Numpy will be used in the treatment of a particular outlier anomaly, in which the `where()` function is used to find observations that satisfy a particular condition, which will then be replaced with null values using `numpy.nan`.\n",
    "    - **missingno**: The `matrix()` function from the **missingno** library will be used to generate a matrix of missing values to confirm that all missing values have been treated, as it provides an efficient means of visualizing the missing data.\n",
    "    - `decomposition` from **sklearn**: sklearn's `decomposition` will be used for Principal Component Analysis.\n",
    "\n",
    "### C4. Annotated Code used to Assess Data Quality\n",
    "The following cells include the annotated code used to assess the quality of the data. See code attached, `D206_PA_MendezD.ipynb`, for the executable script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb67fae-604b-4c28-97a8-7f6ac051ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## C4. The following cells include the annotated code used to assess the quality of the data.\n",
    "# See code attached, in D206_PA_MendezD.ipynb\n",
    "\n",
    "\n",
    "# C1 Import the Pandas library, then load the data into a data frame with Pandas' .read_csv() function\n",
    "import pandas as pd \n",
    "df = pd.read_csv('/Users/drewmendez/Documents/WGU/D206/D206_churndict/churn_raw_data.csv')\n",
    "\n",
    "# C1 Verify that the first unnamed column is identical \n",
    "# to the second column, the 'CaseOrder' variable\n",
    "\n",
    "isFirstEqualtoSecond = df.iloc[:, 0].equals(df['CaseOrder'])\n",
    "\n",
    "# C1 Detect duplicates in the data frame with Pandas' .duplicated() method, \n",
    "# then sum the resulting series with the .sum() method\n",
    "duplicate_count = df.duplicated().sum()\n",
    "\n",
    "# C1 Identify missing values in the data frame with Pandas' .isnull() method, \n",
    "# then sum the resulting series with the .sum() method\n",
    "missing_values_count = df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1cd228-a603-4027-8337-70b19f839ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All quantitative variables assessed for outliers using .boxplot() from Seaborn.\n",
    "# The quantitative variables being assessed:\n",
    "# Lat, Lng, Population, Children, Age, Income, Outage_sec_perweek, Email, Contacts, \n",
    "# Yearly_equip_failure, Tenure, MonthlyCharge, Bandwidth_GB_Year\n",
    "\n",
    "\n",
    "## C1 Detection of 'Lat' Outliers\n",
    "\n",
    "import seaborn as sb\n",
    "\n",
    "# Boxplot to visualize 'Lat' outliers\n",
    "boxplot = sb.boxplot(x = 'Lat', data=df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b366d-eed7-437b-aabe-801efd8ce9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## C1 Detection of 'Lng' Outliers\n",
    "\n",
    "# Boxplot to visualize 'Lng' outliers\n",
    "boxplot = sb.boxplot(x = 'Lng', data=df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92027a9-9356-41bc-92fd-649c8d733827",
   "metadata": {},
   "outputs": [],
   "source": [
    "## C1 Detection of 'Population' Outliers\n",
    "\n",
    "# Boxplot to visualize 'Population' outliers\n",
    "boxplot = sb.boxplot(x = 'Population', data=df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351c859-cc86-4a93-999f-21e63a7607ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## C1 Detection of 'Children' Outliers\n",
    "\n",
    "# Boxplot to visualize 'Children' outliers\n",
    "boxplot = sb.boxplot(x = 'Children', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c46b137-5ed5-4822-8006-2cc2b5ae3b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## C1 Detection of 'Age' Outliers\n",
    "\n",
    "# Boxplot to visualize 'Age' outliers\n",
    "boxplot = sb.boxplot(x = 'Age', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cddbdf-5272-4593-abb8-99efc8e7efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## C1 Detection of 'Income' Outliers\n",
    "\n",
    "# Boxplot to visualize 'Income' outliers\n",
    "boxplot = sb.boxplot(x = 'Income', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f99131a-6be7-4e81-a533-277a066b9a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## C1 Detection of 'Outage_sec_perweek' Outliers\n",
    "\n",
    "# Boxplot to visualize 'Outage_sec_perweek' outliers\n",
    "boxplot = sb.boxplot(x = 'Outage_sec_perweek', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "380bf499-0476-49d7-8258-acbd8ee1e38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## C1 Detection of 'Email' Outliers\n",
    "\n",
    "# Boxplot to visualize 'Email' outliers\n",
    "boxplot = sb.boxplot(x = 'Email', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42e25a-4713-4abd-ae3e-8b4583edd4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "## C1 Detection of 'Contacts' Outliers\n",
    "\n",
    "# Boxplot to visualize 'Contacts' outliers\n",
    "boxplot = sb.boxplot(x = 'Contacts', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a309e3-48c8-4b49-812f-2ab135ade061",
   "metadata": {},
   "outputs": [],
   "source": [
    "## C1 Detection of 'Yearly_equip_failure' Outliers\n",
    "\n",
    "# Boxplot to visualize 'Yearly_equip_failure' outliers\n",
    "boxplot = sb.boxplot(x = 'Yearly_equip_failure', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71c5ea3-bd9b-4861-af42-a54b90b4909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## C1 Detection of 'Tenure' Outliers\n",
    "\n",
    "# Boxplot to visualize 'Tenure' outliers\n",
    "boxplot = sb.boxplot(x = 'Tenure', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc6560-533f-40c3-9274-5ac727995859",
   "metadata": {},
   "outputs": [],
   "source": [
    "## C1 Detection of 'MonthlyCharge' Outliers\n",
    "\n",
    "# Boxplot to visualize 'MonthlyCharge' outliers\n",
    "boxplot = sb.boxplot(x = 'MonthlyCharge', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8619b76d-6baf-479e-ab49-c3713e22a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## C1 Detection of 'Bandwidth_GB_Year' Outliers\n",
    "\n",
    "# Boxplot to visualize 'Bandwidth_GB_Year' outliers\n",
    "boxplot = sb.boxplot(x = 'Bandwidth_GB_Year', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5263e171-86d0-4a24-a9e6-5e73c986f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The following cells include the code used to count the outliers to respond to D1.\n",
    "\n",
    "# Please consider this the annotation for the cells in which the following code is utilized more than once,\n",
    "# in order to avoid redundantly repeating the same annotation in each of the cells below.\n",
    "\n",
    "\n",
    "# Within these cells, the following is utilized:\n",
    "\n",
    "# The variables' first and third quartiles, Q1 and Q3 are found using .quantile() from Pandas,\n",
    "# then the interquartile range, or the difference between Q3 and Q1, is found using IQR = Q3 - Q1.\n",
    "\n",
    "# The upper whisker of the boxplot is found using max = Q3 + 1.5 * IQR.\n",
    "# The lower whisker of the boxplot is found using min = Q1 - 1.5 * IQR.\n",
    "\n",
    "# The .sum() method returns the count of observations greater than the max or less than the min.\n",
    "# The .round() method rounds the outlier count to two decimals.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5df507-dedc-412b-bc24-ffeb81b40ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D1 Counting 'Lat' Outliers \n",
    "\n",
    "Q1 = df['Lat'].quantile(0.25)\n",
    "Q3 = df['Lat'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "max = round(Q3 + 1.5 * IQR, 2)\n",
    "min = round(Q1 - 1.5 * IQR, 2)\n",
    "outlier_count_up = (df['Lat'] > max).sum()\n",
    "outlier_count_low = (df['Lat'] < min).sum()\n",
    "\n",
    "outliers_lat = outlier_count_up + outlier_count_low\n",
    "\n",
    "print('For the Lat variable, all observations greater than', max, 'or less than', min, 'are considered outliers.')\n",
    "print('The count of observations greater than', max,'is', outlier_count_up)\n",
    "print('The count of observations less than', min,'is', outlier_count_low)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e8650-142c-499e-adba-4e5685d0575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D1 Counting 'Lng' Outliers \n",
    "\n",
    "Q1 = df['Lng'].quantile(0.25)\n",
    "Q3 = df['Lng'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "min = round(Q1 - 1.5 * IQR, 2)\n",
    "outliers_lng = (df['Lng'] < min).sum()\n",
    "\n",
    "print('For the Lng variable, all observations less than', min, 'are considered outliers.')\n",
    "print('The count of observations less than', min,'is', outliers_lng)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2699f507-8343-40a4-a6d2-428f5620f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D1 Counting 'Population' Outliers\n",
    "\n",
    "Q1 = df['Population'].quantile(0.25)\n",
    "Q3 = df['Population'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "max = Q3 + 1.5 * IQR\n",
    "outliers_pop = (df['Population'] > max).sum()\n",
    "\n",
    "print('For the Population variable, all observations greater than', max, 'are considered outliers.')\n",
    "print('The count of observations greater than', max,'is', outliers_pop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a668e6e-d56f-4cc8-a3d7-e7e863ca7128",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D1 Counting 'Children' Outliers\n",
    "\n",
    "Q1 = df['Children'].quantile(0.25)\n",
    "Q3 = df['Children'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "max = Q3 + 1.5 * IQR\n",
    "outliers_child = (df['Children'] > max).sum()\n",
    "\n",
    "print('For the Children variable, all observations greater than', max, 'are considered outliers.')\n",
    "print('The count of observations greater than', max,'is', outliers_child)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4951ae-35b9-4cc0-903d-3d6fa47f3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D1 Counting 'Income' Outliers \n",
    "\n",
    "Q1 = df['Income'].quantile(0.25)\n",
    "Q3 = df['Income'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "max = round(Q3 + 1.5 * IQR, 2)\n",
    "outliers_income = (df['Income'] > max).sum()\n",
    "\n",
    "print('For the Income variable, all observations greater than', max, 'are considered outliers.')\n",
    "print('The count of observations greater than', max,'is', outliers_income)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4282a2-f9de-47f6-9277-cb763e7b6e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D1 Counting 'Outage_sec_perweek' Outliers \n",
    "\n",
    "Q1 = df['Outage_sec_perweek'].quantile(0.25)\n",
    "Q3 = df['Outage_sec_perweek'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "max = round(Q3 + 1.5 * IQR, 2)\n",
    "min = round(Q1 - 1.5 * IQR, 2)\n",
    "outlier_count_up = (df['Outage_sec_perweek'] > max).sum()\n",
    "\n",
    "# Since this variable has negative observations, first find the count of observations \n",
    "# both greater or equal to 0 and less than the min, or the acceptable lower outliers\n",
    "outlier_count_low = ((df['Outage_sec_perweek'] >= 0) & (df['Outage_sec_perweek'] < min)).sum()\n",
    "\n",
    "# Then find the count of observations less than 0\n",
    "outlier_count_neg = (df['Outage_sec_perweek'] < 0).sum()\n",
    "\n",
    "outliers_outage = outlier_count_up + outlier_count_low\n",
    "\n",
    "print('For the Outage_sec_perweek variable, all observations greater than', max, 'or less than', min, 'are considered outliers.')\n",
    "print('The count of observations greater than', max,'is', outlier_count_up)\n",
    "print('The count of observations less than', min,'and greater than 0 is', outlier_count_low)\n",
    "print('The count of observations less than 0 is', outlier_count_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d881c97-4b4f-4a39-9021-d66ace86b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D1 Counting 'Email' Outliers \n",
    "\n",
    "Q1 = df['Email'].quantile(0.25)\n",
    "Q3 = df['Email'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "max = round(Q3 + 1.5 * IQR, 2)\n",
    "min = round(Q1 - 1.5 * IQR, 2)\n",
    "outlier_count_up = (df['Email'] > max).sum()\n",
    "outlier_count_low = (df['Email'] < min).sum()\n",
    "\n",
    "outliers_email = outlier_count_up + outlier_count_low\n",
    "\n",
    "print('For the Email variable, all observations greater than', max, 'or less than', min, 'are considered outliers.')\n",
    "print('The count of observations greater than', max,'is', outlier_count_up)\n",
    "print('The count of observations less than', min,'is', outlier_count_low)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057a7a7-fa03-44a1-8fa6-e56812398ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D1 Counting 'Contacts' Outliers \n",
    "\n",
    "Q1 = df['Contacts'].quantile(0.25)\n",
    "Q3 = df['Contacts'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "max = round(Q3 + 1.5 * IQR, 2)\n",
    "outliers_contacts = (df['Contacts'] > max).sum()\n",
    "\n",
    "print('For the Contacts variable, all observations greater than', max, 'are considered outliers.')\n",
    "print('The count of observations greater than', max,'is', outliers_contacts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d758a5-c83d-450c-b7b5-ac3aa96bb682",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D1 Counting 'Yearly_equip_failure' Outliers \n",
    "\n",
    "Q1 = df['Yearly_equip_failure'].quantile(0.25)\n",
    "Q3 = df['Yearly_equip_failure'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "max = round(Q3 + 1.5 * IQR, 2)\n",
    "outliers_failure = (df['Yearly_equip_failure'] > max).sum()\n",
    "\n",
    "print('For the Yearly_equip_failure variable, all observations greater than', max, 'are considered outliers.')\n",
    "print('The count of observations greater than', max,'is', outliers_failure)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1b05ad-bd32-41ff-a0e8-b6bfff716c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D1 Counting 'MonthlyCharge' Outliers \n",
    "\n",
    "Q1 = df['MonthlyCharge'].quantile(0.25)\n",
    "Q3 = df['MonthlyCharge'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "max = round(Q3 + 1.5 * IQR, 2)\n",
    "outliers_monthly = (df['MonthlyCharge'] > max).sum()\n",
    "\n",
    "print('For the MonthlyCharge variable, all observations greater than', max, 'are considered outliers.')\n",
    "print('The count of observations greater than', max,'is', outliers_monthly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7377714e-e601-4ff0-ad5c-bf651745d28a",
   "metadata": {},
   "source": [
    "# Part III: Data Cleaning\n",
    "\n",
    "\n",
    "## D. Summary of the Data-Cleaning Process\n",
    "\n",
    "\n",
    "### D1. Description of Data Quality Issues \n",
    "- Duplicates: according to the output of the code below, as determined in C4, it is true that the first unnamed column is identical to the second column, the 'CaseOrder' variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b2bdf-49f8-4ef2-aa4e-a6d2a8f9dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Duplicate columns, as found in C4\n",
    "\n",
    "print('Is the first column equal to the second column?', isFirstEqualtoSecond)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c35d63-2002-4005-adae-cedfa063a997",
   "metadata": {},
   "source": [
    "- Duplicates: according to the output of the code below, as determined in C4, there were no duplicate observations detected in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b5268a-ed3a-4ab2-a156-5c671a8276b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D1. Duplicate observations, as found in C4\n",
    "\n",
    "print('Number of duplicate rows:', duplicate_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d32730-ea19-47ea-bc76-48cfd6d0ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D1. Missing Values, as found in C4\n",
    "\n",
    "print(\"Number of missing values per variable:\")\n",
    "print(missing_values_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa0f884-c32b-4120-a9b2-edb754638fa7",
   "metadata": {},
   "source": [
    "- Outliers:\n",
    "    - The number of outliers in the ‘Lat’ variable: 158\n",
    "    - The number of outliers in the ‘Lng’ variable: 273\n",
    "    - The number of outliers in the ‘Population’ variable: 937\n",
    "    - The number of outliers in the ‘Children’ variable: 302\n",
    "    - The number of outliers in the ‘Income’ variable: 249\n",
    "    - The number of outliers in the ‘Outage_sec_perweek’ variable: 539, 11 of which are negative\n",
    "    - The number of outliers in the ‘Email’ variable: 38\n",
    "    - The number of outliers in the ‘Contacts’ variable: 8\n",
    "    - The number of outliers in the ‘Yearly_equip_failure’ variable: 94\n",
    "    - The number of outliers in the ‘MonthlyCharge’ variable: 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c43ed-ddc3-4bed-817d-e860b87818cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D1. Outliers\n",
    "\n",
    "print('The number of outliers in the ‘Lat’ variable:', outliers_lat)\n",
    "print('The number of outliers in the ‘Lng’ variable:', outliers_lng)\n",
    "print('The number of outliers in the ‘Population’ variable:', outliers_pop)\n",
    "print('The number of outliers in the ‘Children’ variable:', outliers_child)\n",
    "print('The number of outliers in the ‘Income’ variable:', outliers_income)\n",
    "print('The number of outliers in the ‘Outage_sec_perweek’ variable:', outliers_outage)\n",
    "print('The number of negative values in the ‘Outage_sec_perweek’ variable is', outlier_count_neg)\n",
    "print('The number of outliers in the ‘Email’ variable:', outliers_email)\n",
    "print('The number of outliers in the ‘Contacts’ variable:', outliers_contacts)\n",
    "print('The number of outliers in the ‘Yearly_equip_failure’ variable:', outliers_failure)\n",
    "print('The number of outliers in the ‘MonthlyCharge’ variable:', outliers_monthly)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d6b63e-aad9-4e99-945e-d2fcce53c473",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### D2. Justification for Methods Used to Mitigate Data Quality Issues\n",
    "- Duplicates:\n",
    "    - Rows: Since there were no duplicate rows, treatment was concluded upon verification that there were no duplicates.\n",
    "    - Variables: According to the data dictionary, there should be one index variable, the 'CaseOrder' variable. Upon inspection, it was observed that there were actually two index variables, one of which was unnamed. It was shown that these two variables were identical, then the unnamed variable was dropped from the data frame.\n",
    "- Missing values: Missing values were imputed with the median if the distribution of the variable was skewed or bimodal, imputed with the mean if the distribution was normal or uniform, or imputed with the mode if the variable was categorical. (Middleton, K.)\n",
    "    - 'InternetService': Since the 'InternetService' variable has 'None' as one of its options, these erroneously identified null values were imputed with 'None' to avoid being interpreted as nulls.\n",
    "    - 'Children' missing values: Since the distribution of the 'Children' variable is skewed right, missing values were imputed with the median.\n",
    "    - 'Age' missing values: Since the 'Age' variable has a uniform distribution, missing values were imputed with the mean.\n",
    "    - 'Income' missing values: Since the distribution of the 'Income' variable is skewed right, missing values were imputed with the median.\n",
    "    - 'Techie' missing values: Since the 'Techie' variable is categorical, missing values were imputed with the mode.\n",
    "    - 'Phone' missing values: Since the 'Phone' variable is categorical, missing values were imputed with the mode.\n",
    "    - 'TechSupport' missing values: Since the 'TechSupport' variable is categorical, missing values were imputed with the mode.\n",
    "    - 'Tenure' missing values: Since the 'Tenure' variable has a bimodal distribution, missing values were imputed with the median.\n",
    "    - 'Bandwidth_GB_Year' missing values: Since the 'Bandwidth_GB_Year' variable has a bimodal distribution, missing values were imputed with the median.\n",
    "- Outliers:\n",
    "    - 'Lat' outliers: Since these outliers were observed to be in the acceptable range for the variable, they were retained.\n",
    "    - 'Lng' outliers: Since these outliers were observed to be in the acceptable range for the variable, they were retained.\n",
    "    - 'Population' outliers: Since these outliers were observed to be in the acceptable range for the variable, they were retained.\n",
    "    - 'Children' outliers: Since these outliers were observed to be in the acceptable range for the variable, they were retained.\n",
    "    - 'Income' outliers: Since these outliers were observed to be in the acceptable range for the variable, they were retained.\n",
    "    - 'Outage_sec_perweek' outliers: 539 of these outliers were observed.\n",
    "        - Only 528 were acceptable and retained, while 11 of them were negative, which was unreasonable, as these should be positive values. These were imputed with with NANs using `numpy.nan`, and since the distribution was skewed right, these were then imputed with the median.\n",
    "    - 'Email' outliers: Since these outliers were observed to be in the acceptable range for the variable, they were retained.\n",
    "    - 'Contacts' outliers: Since these outliers were observed to be in the acceptable range for the variable, they were retained.\n",
    "    - 'Yearly_equip_failure' outliers: Since these outliers were observed to be in the acceptable range for the variable, they were retained.\n",
    "    - 'MonthlyCharge' outliers: Since these outliers were observed to be in the acceptable range for the variable, they were retained.\n",
    "- Re-expression:\n",
    "    - 'Education': Since the 'Education' variable is an ordinal categorical variable, it was re-expressed using numeric values in a new column, 'Education_numeric', using the following mapping:\n",
    "        - Master's Degree: 18\n",
    "        - Regular High School Diploma: 12\n",
    "        - Doctorate Degree: 20\n",
    "        - No Schooling Completed: 0\n",
    "        - Associate's Degree: 14\n",
    "        - Bachelor's Degree: 16\n",
    "        - Some College, Less than 1 year: 13\n",
    "        - GED or Alternative Credential: 12\n",
    "        - Some College, 1 or More Years, No Degree: 14\n",
    "        - 9th Grade to 12th Grade, No Diploma: 11\n",
    "        - Nursery School to 8th Grade: 8\n",
    "        - Professional School Degree: 18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0704e03e-8fe7-4593-bf60-a92a3768345d",
   "metadata": {},
   "source": [
    "### D3. Summary of the Outcome of Implementing Each Data Cleaning Step\n",
    "**Detection**  \n",
    "- Duplicate columns and observations were detected. The potential duplicate column was verified to be identical to the 'CaseOrder' variable using Pandas' `.equals()` method. The data set was then checked for duplicate observations by calling Pandas' `.duplicated()` method on the data frame. The resulting series was then summed using the `.sum()` method to determine the count of all duplicate observations.\n",
    "- Missing values of each variable were detected by calling Pandas' `.isnull()` method on the data frame. The resulting series was then summed using the `.sum()` method to determine the count of all missing values.\n",
    "- Outliers were visually detected using Seaborn's `boxplot()` function. If a variable was found to have outliers, the number of outliers was determined. In order to count the outliers, it was necessary to calculate the values of the boxplot's upper and lower whiskers. Then the upper outliers were found to be greater than the value of the upper whisker, and lower outliers were found to be less than the value of the lower whisker. The total counts of outliers were then stored in appropriately named variables.\n",
    "  \n",
    "**Treatment**  \n",
    "- The duplicate column, unnamed column, was dropped by calling Pandas' `.drop()` method on the data frame. This resulted in a data frame of reduced size, with one less column.\n",
    "- No duplicate observations were detected, so the data frame remained unchanged after performing this step.\n",
    "- The distributions of variables containing missing values were observed using Matplotlib's `plot()` function to generate histograms of each variable. Based on each distribution, the missing values that were detected were then imputed using univariate imputation, utilizing Pandas' `.fillna()` method chained with the appropriate statistical measure. A histogram of each newly treated variable was then generated in order to verify that the distribution had not been drastically changed.\n",
    "- Outliers were inspected and gauged against the data dictionary and range of expected values to determine the course of action. All but one variable had outliers that were determined to be reasonable, and as such, were retained.\n",
    "- One variable, 'Outage_sec_perweek', was found to have negative values, which were outside the range of what was considered reasonable for that variable. These negative values were isolated, converted to nulls, and then imputed with the median.\n",
    "\n",
    "\n",
    "## D3. Visual Evidence Confirming Data is Cleaned\n",
    "This section has been moved to the end of section D4 in order to execute it in the proper order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dce310-87ea-4f7f-b60c-cf404f22fb48",
   "metadata": {},
   "source": [
    "## D4. Annotated Code used to Mitigate Data Quality Issues\n",
    "\n",
    "The following cells include the annotated code used to mitigate data quality issues, which includes treating duplicates, missing values, outliers, as well as addressing anomalies. See code attached, `D206_PA_MendezD.ipynb`, for the executable script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30171bd5-010b-45d1-b3b6-80b48b935703",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D4. The following cells include the annotated code used to mitigate the data quality issues.\n",
    "# See code attached, in D206_PA_MendezD.ipynb\n",
    "\n",
    "# Before dropping the first unnamed column, first verify that it is identical \n",
    "# to the second column, the 'CaseOrder' variable\n",
    "\n",
    "print('Is the first column equal to the second column?', isFirstEqualtoSecond)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1e692-211d-4ee2-ad55-eac6f60e348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D4. The following cells include the annotated code used to mitigate the data quality issues.\n",
    "# See code attached, in D206_PA_MendezD.ipynb\n",
    "\n",
    "\n",
    "# Since the first unnamed column and the second column, the 'CaseOrder' variable are identical, \n",
    "# proceed with dropping the unnamed column\n",
    "\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "# Verify the unnamed column was dropped\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f499ea01-1c31-4829-901e-64e9fb456b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a histogram of the 'Children' variable to observe distribution\n",
    "df['Children'].plot(kind='hist')\n",
    "\n",
    "# Display the histogram to observe the distribution of the variable\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f966a97-50e3-419b-848d-5078a5d965ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the 'Children' variable is skewed right, impute with the median\n",
    "\n",
    "# Call the data frame and variable, then fill all NAs of that variable with its median\n",
    "df['Children'].fillna(df['Children'].median(), inplace=True)\n",
    "\n",
    "# Verify that all nulls are treated \n",
    "print('Number of nulls:', df['Children'].isnull().sum())\n",
    "\n",
    "# Generate a new histogram of the 'Children' variable to observe unchanged distribution\n",
    "df['Children'].plot(kind='hist')\n",
    "\n",
    "# Display the histogram \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb31ce53-7550-49ac-9032-705dc965692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram of the 'Age' variable to observe distribution\n",
    "df['Age'].plot(kind='hist')\n",
    "\n",
    "# Display the histogram to observe the distribution of the variable\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397a13a5-2bdf-4ebb-b7c1-f955d2ad6a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the 'Age' variable has a uniform distribution, impute with the mean.\n",
    "\n",
    "# Call the data frame and variable, then fill all NAs of that variable with its mean\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)\n",
    "\n",
    "# Verify that all nulls are treated \n",
    "print('Number of nulls:', df['Age'].isnull().sum())\n",
    "\n",
    "# Generate a new histogram of the 'Age' variable to observe unchanged distribution\n",
    "df['Age'].plot(kind='hist')\n",
    "\n",
    "# Display the histogram to observe the distribution of the variable\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1464255-5ca9-4eaa-93f9-fe72eee13d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram of the 'Income' variable to observe distribution\n",
    "df['Income'].plot(kind='hist')\n",
    "\n",
    "# Display the histogram to observe the distribution of the variable\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c6bc6c-124c-4f60-b32f-d41214770e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the 'Income' variable is skewed right, impute with the median\n",
    "\n",
    "# Call the data frame and variable, then fill all NAs of that variable with its median\n",
    "df['Income'].fillna(df['Income'].median(), inplace=True)\n",
    "\n",
    "# Verify that all nulls are treated \n",
    "print('Number of nulls:', df['Income'].isnull().sum())\n",
    "\n",
    "# Generate a new histogram of the 'Income' variable to observe unchanged distribution\n",
    "df['Income'].plot(kind='hist')\n",
    "\n",
    "# Display the histogram \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c394900-50fc-48e9-b26a-27775b9732d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the 'Techie' variable is categorical, impute with the mode\n",
    "\n",
    "# Call the data frame and variable, then fill all NAs of that variable with its mode\n",
    "df['Techie'].fillna(df['Techie'].mode()[0], inplace=True)\n",
    "\n",
    "# Verify that all nulls are treated \n",
    "print('Number of nulls:', df['Techie'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edbe654-1786-4fde-a0ec-17301f30fa95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the 'Phone' variable is categorical, impute with the mode\n",
    "\n",
    "# Call the data frame and variable, then fill all NAs of that variable with its mode\n",
    "df['Phone'].fillna(df['Phone'].mode()[0], inplace=True)\n",
    "\n",
    "# Verify that all nulls are treated \n",
    "print('Number of nulls:', df['Phone'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd98b690-c69b-4353-a4fc-a2e8301cfca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the 'TechSupport' variable is categorical, impute with the mode\n",
    "\n",
    "# Call the data frame and variable, then fill all NAs of that variable with its mode\n",
    "df['TechSupport'].fillna(df['TechSupport'].mode()[0], inplace=True)\n",
    "\n",
    "# Verify that all nulls are treated \n",
    "print('Number of nulls:', df['TechSupport'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50a3b58-2ed1-4f2b-a146-6e0530fe0115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram of the 'Tenure' variable\n",
    "df['Tenure'].plot(kind='hist')\n",
    "\n",
    "# Display the histogram to observe the distribution of the variable\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3226b00-db82-4219-9161-240471d19a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the 'Tenure' variable is bimodal, impute with the median\n",
    "\n",
    "# Call the data frame and variable, then fill all NAs of that variable with its median\n",
    "df['Tenure'].fillna(df['Tenure'].median(), inplace=True)\n",
    "\n",
    "# Verify that all nulls are treated \n",
    "print('Number of nulls:', df['Tenure'].isnull().sum())\n",
    "\n",
    "# Generate a new histogram of the 'Tenure' variable to observe unchanged distribution\n",
    "df['Tenure'].plot(kind='hist')\n",
    "\n",
    "# Display the histogram to observe unchanged distribution\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c5b0d5-7c47-448f-bb93-6247b69c54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a histogram of the 'Bandwidth_GB_Year' variable\n",
    "df['Bandwidth_GB_Year'].plot(kind='hist')\n",
    "\n",
    "# Display the histogram to observe the distribution of the variable\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1b26a1-0e27-4bdb-81e0-696e62d17116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the 'Bandwidth_GB_Year' variable is bimodal, impute with the median\n",
    "\n",
    "# Call the data frame and variable, then fill all NAs of that variable with its median\n",
    "df['Bandwidth_GB_Year'].fillna(df['Bandwidth_GB_Year'].median(), inplace=True)\n",
    "\n",
    "# Verify that all nulls are treated \n",
    "print('Number of nulls:', df['Bandwidth_GB_Year'].isnull().sum())\n",
    "\n",
    "# Generate a new histogram of the 'Bandwidth_GB_Year' variable to observe unchanged distribution\n",
    "df['Bandwidth_GB_Year'].plot(kind='hist')\n",
    "\n",
    "# Display the histogram \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ad261a-e8f6-4c8d-a9b8-11b5c0ee52f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the 'InternetService' variable has 'None' as one of its options,\n",
    "# it is necessary to impute 'None' \n",
    "\n",
    "df['InternetService'].fillna('None', inplace=True)\n",
    "\n",
    "# Verify that 'None' no longer appears as 'Null'\n",
    "print('Number of nulls:', df['Tenure'].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca81ce28-9b54-4dce-8d0c-cd3a4b818d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the 'Outage_sec_perweek' variable had negative outliers,\n",
    "# these must be converted to nulls and then treated accordingly.\n",
    "\n",
    "# Generate a histogram of the 'Outage_sec_perweek' variable\n",
    "df['Outage_sec_perweek'].plot(kind='hist')\n",
    "\n",
    "# Display the histogram to observe the distribution of the variable\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff769bd-b050-4628-ab07-4b581f5336b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the 'Outage_sec_perweek' distribution is skewed right, \n",
    "# impute negative outliers with the median\n",
    "import numpy as np\n",
    "\n",
    "# Replace negative outliers with nulls using np.nan\n",
    "df['Outage_sec_perweek'] = np.where(df['Outage_sec_perweek'] < 0, np.nan, df['Outage_sec_perweek'])\n",
    "\n",
    "# Use fillna() to impute with the median\n",
    "df['Outage_sec_perweek'].fillna(df['Outage_sec_perweek'].median(), inplace=True)\n",
    "\n",
    "# Generate a new histogram of the 'Outage_sec_perweek' variable\n",
    "df['Outage_sec_perweek'].plot(kind='hist')\n",
    "\n",
    "# Display the histogram to observe the unchanged distribution of the variable\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a877db-6823-4f52-ac81-3148a486ab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine all unique values for 'Education' variable\n",
    "df.Education.unique() \n",
    "\n",
    "# Make a copy of the variable in the data frame\n",
    "df['Education_numeric'] = df['Education']\n",
    "\n",
    "# Create a dictionary assigning numeric values to each categorical value\n",
    "dict_edu = {\"Education_numeric\": {\"Master's Degree\":18, \"Regular High School Diploma\":12, \n",
    "    \"Doctorate Degree\":20, \"No Schooling Completed\":0, \"Associate's Degree\":14,\n",
    "    \"Bachelor's Degree\":16, \"Some College, Less than 1 Year\":13,\n",
    "    \"GED or Alternative Credential\":12, \"Some College, 1 or More Years, No Degree\":14,\n",
    "    \"9th Grade to 12th Grade, No Diploma\":11, \"Nursery School to 8th Grade\":8, \"Professional School Degree\":18}}\n",
    "\n",
    "# Use the dictionary to replace values of the the variable\n",
    "df.replace(dict_edu, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1bde94-afc5-449a-8b93-1a68176770c2",
   "metadata": {},
   "source": [
    "## D3. Visual Evidence Confirming Data is Cleaned\n",
    "\n",
    "- Using the `matrix()` function from the **missingno** library is used here to generate a matrix of missing values. It is clear that all missing values have been treated.\n",
    "- By creating a `boxplot()` of the 'Outage_sec_perweek' variable, it is apparent that the negative outliers have been treated.\n",
    "- By calling the `.unique()` method on the newly created 'Education_numeric' variable, it is clear that the categorical options have been re-mapped to numeric values using ordinal encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db21c79-b92e-44aa-bbb1-b0f3e3d20b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D3. Visual Evidence Confirming Data is Cleaned: Missing Values\n",
    "\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "msno.matrix(df, fontsize = 12, labels=True)\n",
    "plt.title('Missing data matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7276d-e1ee-400f-86cd-003f023e8611",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## D3. Visual Evidence Confirming Data is Cleaned: Unreasonable Outliers\n",
    "\n",
    "# Boxplot to visualize 'Outage_sec_perweek' outliers\n",
    "boxplot = sb.boxplot(x = 'Outage_sec_perweek', data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2eb775-eab3-401d-a7b6-6141726f530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## D3. Visual Evidence Confirming Data is Cleaned: Re-Expression of Categorical Variables\n",
    "\n",
    "ed_cats = df.Education_numeric.unique() \n",
    "print('The numerically coded levels of education are:', ed_cats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3df4b9a-1f8a-40c7-b6e5-6ee5613d7fd0",
   "metadata": {},
   "source": [
    "### D5. Copy of Cleaned Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bfb042-1843-4223-9fd5-bedf2a2f1a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code used to export .csv of cleaned data\n",
    "\n",
    "df.to_csv('D206_PA_MendezD_cleaned.csv', sep=',', encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c787aea-9a26-4f78-933e-d15909ae48ea",
   "metadata": {},
   "source": [
    "### D6. Summarize the Limitations of the Data-Cleaning Process\n",
    "Since the imputed values may not accurately represent the true values, data imputation, like used in this data-cleaning process, can introduce uncertainty into the data set. The choice of statistical measure used for imputation can also impact the analysis. The retention of outliers could potentially distort summary statistics like the mean and standard deviation. These retained outliers may also skew the interpretation of relationships within the data.\n",
    "\n",
    "### D7. How the Above Limitations May Affect the Analysis\n",
    "Retention of outliers and imputed values could potentially mislead the decision-making process, as well as misinform the insights provided by the model. The outliers may also obscure the interpretation of results and make it difficult it draw meaningful conclusions from the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83814ae6-ab26-476a-bcc1-170196fbfc97",
   "metadata": {},
   "source": [
    "## E. Applying Principal Component Analysis\n",
    "\n",
    "### E1. Total Number of Principal Components and the Output of the Principal Components Loading Matrix\n",
    "\n",
    "The variables used to perform PCA were the eight continuous variables from the data set:  \n",
    "\n",
    "- Lat\n",
    "- Lng\n",
    "- Age\n",
    "- Income\n",
    "- Outage_sec_perweek\n",
    "- Tenure\n",
    "- MonthlyCharge\n",
    "- Bandwidth_GB_Year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930c22cc-11c0-431a-be33-981325c140e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from K. Middleton's PCA Webinar\n",
    "\n",
    "\n",
    "# Create data frame with the continuous variables to be used for PCA\n",
    "pca_frame = df[['Lat', 'Lng', 'Age', 'Income', 'Outage_sec_perweek', 'Tenure', 'MonthlyCharge', 'Bandwidth_GB_Year']]\n",
    "\n",
    "# Apply z-score normalization for each column of the data frame\n",
    "pca_frame_norm = (pca_frame - pca_frame.mean()) / pca_frame.std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f41c6-a118-48b8-984c-050ccb2bc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from K. Middleton's PCA Webinar\n",
    "\n",
    "\n",
    "## Apply PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# call the PCA function, shape the data based upon the number of PCs created\n",
    "pca = PCA(n_components = pca_frame.shape[1]) \n",
    "\n",
    "# fit the PCA on the normalized data set \n",
    "pca.fit(pca_frame_norm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c65d09-b936-4926-82ea-3b38c157df6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from K. Middleton's PCA Webinar\n",
    "\n",
    "\n",
    "# Transform into data frame\n",
    "df_pca = pd.DataFrame(pca.transform(pca_frame_norm), columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8'])\n",
    "\n",
    "loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5', 'PC6', 'PC7', 'PC8'], index=pca_frame.columns)\n",
    "loadings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c900fab8-8bb9-473e-85b1-5bbc8cea7c20",
   "metadata": {},
   "source": [
    "### E2. Justification for Reduced Number of Principal Components with Scree Plot\n",
    "\n",
    "According to the Kaiser Rule, we retain PCs with eigenvalues greater than or equal to 1. (Middleton, K.)\n",
    "\n",
    "The scree plot below shows that five PCs have eigenvalues greater than or equal to 1:  \n",
    "\n",
    "- PC1\n",
    "- PC2\n",
    "- PC3\n",
    "- PC4\n",
    "- PC5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318ad0a-0d51-456d-b0e1-72e85bacbabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from K. Middleton's PCA Webinar\n",
    "\n",
    "\n",
    "# Calculate covariance and vectors then define eigenvalues before creating scree plot\n",
    "\n",
    "# Create a covariance matrix\n",
    "cov_matrix = np.dot(pca_frame_norm.T, pca_frame_norm) / pca_frame.shape[0]\n",
    "\n",
    "# Create and store the eigenvectors in a new data frame\n",
    "eigenvalues = [np.dot(eigenvector.T, np.dot(cov_matrix, eigenvector)) for eigenvector in pca.components_]\n",
    "\n",
    "# Plot eigenvalues\n",
    "plt.plot(np.arange(1, len(eigenvalues)+1), eigenvalues, marker = 'o')\n",
    "plt.xlabel('Principal Component Number')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.axhline(y=1, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094ab229-0080-4104-800c-d8f7bfac06d5",
   "metadata": {},
   "source": [
    "### E3. Benefits to the Organization from PCA\n",
    "\n",
    "Principal Component Analysis can help to reduce the dimensionality of the data set by transforming the original continuous variables into a smaller set of variables called principal components. This can simplify the analysis and visualization of data, making it easier for the organization to interpret and understand the findings (Bigabid, 2023)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d4a24-84c4-4182-b3b6-94ee7aca2153",
   "metadata": {},
   "source": [
    "# Part IV. Supporting Documents\n",
    "\n",
    "\n",
    "### F. Panopto Video Demonstrating the Code Functionality\n",
    "https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=c71d2f83-8164-44fd-8ab9-b14f012326f1\n",
    "\n",
    "### G. Acknowledgement of Web Sources\n",
    "Middleton, K. *D206 - Getting Started with D206 | PCA* [Webinar]. Western Governors University. https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=3bcc452f-fa35-43be-b69f-b05901356f95  \n",
    "\n",
    "### H. Acknowledgement of Sources\n",
    "Middleton, K. *D206 - Getting Started with D206 | Missing Values* [Webinar]. Western Governors University. https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=767749d2-ba19-4f94-bec8-b058017b2f5e  \n",
    "\n",
    "Middleton, K. *D206 - Getting Started with D206 | PCA* [Webinar]. Western Governors University. https://wgu.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=3bcc452f-fa35-43be-b69f-b05901356f95  \n",
    "\n",
    "*What is Principal Component Analysis and How Can I Use It?*. Bigabid. (2023, February 8). https://www.bigabid.com/what-is-pca-and-how-can-i-use-it/  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
